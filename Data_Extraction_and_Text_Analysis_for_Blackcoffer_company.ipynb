{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"nbformat\": 4,\n",
    "  \"nbformat_minor\": 0,\n",
    "  \"metadata\": {\n",
    "    \"colab\": {\n",
    "      \"provenance\": []\n",
    "    },\n",
    "    \"kernelspec\": {\n",
    "      \"name\": \"python3\",\n",
    "      \"display_name\": \"Python 3\"\n",
    "    },\n",
    "    \"language_info\": {\n",
    "      \"name\": \"python\"\n",
    "    },\n",
    "    \"gpuClass\": \"standard\"\n",
    "  },\n",
    "  \"cells\": [\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 1,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"Qa_ONewhIVLB\",\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\"\n",
    "        },\n",
    "        \"outputId\": \"7acfdd07-ca22-4611-fdea-6e64e7d6c0c5\"\n",
    "      },\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"output_type\": \"stream\",\n",
    "          \"name\": \"stdout\",\n",
    "          \"text\": [\n",
    "            \"Mounted at /gdrive\\n\",\n",
    "            \"/gdrive/MyDrive/project/Data_Extraction_and_NLP/TestAssignment\\n\"\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"source\": [\n",
    "        \"from google.colab import drive\\n\",\n",
    "        \"drive.mount('/gdrive')\\n\",\n",
    "        \"%cd '/gdrive/MyDrive/project/Data_Extraction_and_NLP/TestAssignment'\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"#import necessary pacakages\\n\",\n",
    "        \"import requests\\n\",\n",
    "        \"from bs4 import BeautifulSoup\\n\",\n",
    "        \"import pandas as pd\\n\",\n",
    "        \"import os\\n\",\n",
    "        \"import nltk\\n\",\n",
    "        \"from nltk.tokenize import word_tokenize\\n\",\n",
    "        \"from nltk.corpus import stopwords\\n\",\n",
    "        \"nltk.download('punkt')\\n\",\n",
    "        \"nltk.download('stopwords')\\n\",\n",
    "        \"import re\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\"\n",
    "        },\n",
    "        \"id\": \"3lNMH1gfF3cF\",\n",
    "        \"outputId\": \"3474f221-df66-44ae-82b0-0123be61adae\"\n",
    "      },\n",
    "      \"execution_count\": 26,\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"output_type\": \"stream\",\n",
    "          \"name\": \"stderr\",\n",
    "          \"text\": [\n",
    "            \"[nltk_data] Downloading package punkt to /root/nltk_data...\\n\",\n",
    "            \"[nltk_data]   Package punkt is already up-to-date!\\n\",\n",
    "            \"[nltk_data] Downloading package stopwords to /root/nltk_data...\\n\",\n",
    "            \"[nltk_data]   Package stopwords is already up-to-date!\\n\"\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"\\n\",\n",
    "        \"#read the url file into the pandas object\\n\",\n",
    "        \"df = pd.read_excel('Input.xlsx')\\n\",\n",
    "        \"\\n\",\n",
    "        \"#loop throgh each row in the df\\n\",\n",
    "        \"for index, row in df.iterrows():\\n\",\n",
    "        \"  url = row['URL']\\n\",\n",
    "        \"  url_id = row['URL_ID']\\n\",\n",
    "        \"\\n\",\n",
    "        \"  # make a request to url\\n\",\n",
    "        \"  header = {'User-Agent': \\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\\\"}\\n\",\n",
    "        \"  try:\\n\",\n",
    "        \"    response = requests.get(url,headers=header)\\n\",\n",
    "        \"  except:\\n\",\n",
    "        \"    print(\\\"can't get response of {}\\\".format(url_id))\\n\",\n",
    "        \"\\n\",\n",
    "        \"  #create a beautifulsoup object\\n\",\n",
    "        \"  try:\\n\",\n",
    "        \"    soup = BeautifulSoup(response.content, 'html.parser')\\n\",\n",
    "        \"  except:\\n\",\n",
    "        \"    print(\\\"can't get page of {}\\\".format(url_id))\\n\",\n",
    "        \"  #find title\\n\",\n",
    "        \"  try:\\n\",\n",
    "        \"    title = soup.find('h1').get_text()\\n\",\n",
    "        \"  except:\\n\",\n",
    "        \"    print(\\\"can't get title of {}\\\".format(url_id))\\n\",\n",
    "        \"    continue\\n\",\n",
    "        \"  #find text\\n\",\n",
    "        \"  article = \\\"\\\"\\n\",\n",
    "        \"  try:\\n\",\n",
    "        \"    for p in soup.find_all('p'):\\n\",\n",
    "        \"      article += p.get_text()\\n\",\n",
    "        \"  except:\\n\",\n",
    "        \"    print(\\\"can't get text of {}\\\".format(url_id))\\n\",\n",
    "        \"\\n\",\n",
    "        \"  #write title and text to the file\\n\",\n",
    "        \"  file_name = '/gdrive/MyDrive/project/Data_Extraction_and_NLP/TestAssignment/TitleText/' + str(url_id) + '.txt'\\n\",\n",
    "        \"  with open(file_name, 'w') as file:\\n\",\n",
    "        \"    file.write(title + '\\\\n' + article)\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\"\n",
    "        },\n",
    "        \"id\": \"AXXMNkOohRgr\",\n",
    "        \"outputId\": \"0de27ec4-58a7-4a93-f0ff-e5df6ed9c1f3\"\n",
    "      },\n",
    "      \"execution_count\": 23,\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"output_type\": \"stream\",\n",
    "          \"name\": \"stdout\",\n",
    "          \"text\": [\n",
    "            \"can't get title of 44.0\\n\",\n",
    "            \"can't get title of 57.0\\n\",\n",
    "            \"can't get title of 144.0\\n\"\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"\\n\",\n",
    "        \"# Directories\\n\",\n",
    "        \"text_dir = \\\"/gdrive/MyDrive/project/Data_Extraction_and_NLP/TestAssignment/TitleText\\\"\\n\",\n",
    "        \"stopwords_dir = \\\"/gdrive/MyDrive/project/Data_Extraction_and_NLP/TestAssignment/StopWords\\\"\\n\",\n",
    "        \"sentment_dir = \\\"/gdrive/MyDrive/project/Data_Extraction_and_NLP/TestAssignment/MasterDictionary\\\"\\n\",\n",
    "        \"\\n\",\n",
    "        \"# load all stop wors from the stopwords directory and store in the set variable\\n\",\n",
    "        \"stop_words = set()\\n\",\n",
    "        \"for files in os.listdir(stopwords_dir):\\n\",\n",
    "        \"  with open(os.path.join(stopwords_dir,files),'r',encoding='ISO-8859-1') as f:\\n\",\n",
    "        \"    stop_words.update(set(f.read().splitlines()))\\n\",\n",
    "        \"\\n\",\n",
    "        \"# load all text files  from the  directory and store in a list(docs)\\n\",\n",
    "        \"docs = []\\n\",\n",
    "        \"for text_file in os.listdir(text_dir):\\n\",\n",
    "        \"  with open(os.path.join(text_dir,text_file),'r') as f:\\n\",\n",
    "        \"    text = f.read()\\n\",\n",
    "        \"#tokenize the given text file\\n\",\n",
    "        \"    words = word_tokenize(text)\\n\",\n",
    "        \"# remove the stop words from the tokens\\n\",\n",
    "        \"    filtered_text = [word for word in words if word.lower() not in stop_words]\\n\",\n",
    "        \"# add each filtered tokens of each file into a list\\n\",\n",
    "        \"    docs.append(filtered_text)\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"# store positive, Negative words from the directory\\n\",\n",
    "        \"pos=set()\\n\",\n",
    "        \"neg=set()\\n\",\n",
    "        \"\\n\",\n",
    "        \"for files in os.listdir(sentment_dir):\\n\",\n",
    "        \"  if files =='positive-words.txt':\\n\",\n",
    "        \"    with open(os.path.join(sentment_dir,files),'r',encoding='ISO-8859-1') as f:\\n\",\n",
    "        \"      pos.update(f.read().splitlines())\\n\",\n",
    "        \"  else:\\n\",\n",
    "        \"    with open(os.path.join(sentment_dir,files),'r',encoding='ISO-8859-1') as f:\\n\",\n",
    "        \"      neg.update(f.read().splitlines())\\n\",\n",
    "        \"\\n\",\n",
    "        \"# now collect the positive  and negative words from each file\\n\",\n",
    "        \"# calculate the scores from the positive and negative words \\n\",\n",
    "        \"positive_words = []\\n\",\n",
    "        \"Negative_words =[]\\n\",\n",
    "        \"positive_score = []\\n\",\n",
    "        \"negative_score = []\\n\",\n",
    "        \"polarity_score = []\\n\",\n",
    "        \"subjectivity_score = []\\n\",\n",
    "        \"\\n\",\n",
    "        \"#iterate through the list of docs\\n\",\n",
    "        \"for i in range(len(docs)):\\n\",\n",
    "        \"  positive_words.append([word for word in docs[i] if word.lower() in pos])\\n\",\n",
    "        \"  Negative_words.append([word for word in docs[i] if word.lower() in neg])\\n\",\n",
    "        \"  positive_score.append(len(positive_words[i]))\\n\",\n",
    "        \"  negative_score.append(len(Negative_words[i]))\\n\",\n",
    "        \"  polarity_score.append((positive_score[i] - negative_score[i]) / ((positive_score[i] + negative_score[i]) + 0.000001))\\n\",\n",
    "        \"  subjectivity_score.append((positive_score[i] + negative_score[i]) / ((len(docs[i])) + 0.000001))\\n\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"1tRdSv8ErMOm\"\n",
    "      },\n",
    "      \"execution_count\": 24,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Average Sentence Length = the number of words / the number of sentences\\n\",\n",
    "        \"# Percentage of Complex words = the number of complex words / the number of words \\n\",\n",
    "        \"# Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\\n\",\n",
    "        \"\\n\",\n",
    "        \"avg_sentence_length = []\\n\",\n",
    "        \"Percentage_of_Complex_words  =  []\\n\",\n",
    "        \"Fog_Index = []\\n\",\n",
    "        \"complex_word_count =  []\\n\",\n",
    "        \"avg_syllable_word_count =[]\\n\",\n",
    "        \"\\n\",\n",
    "        \"stopwords = set(stopwords.words('english'))\\n\",\n",
    "        \"def measure(file):\\n\",\n",
    "        \"  with open(os.path.join(text_dir, file),'r') as f:\\n\",\n",
    "        \"    text = f.read()\\n\",\n",
    "        \"# remove punctuations \\n\",\n",
    "        \"    text = re.sub(r'[^\\\\w\\\\s.]','',text)\\n\",\n",
    "        \"# split the given text file into sentences\\n\",\n",
    "        \"    sentences = text.split('.')\\n\",\n",
    "        \"# total number of sentences in a file\\n\",\n",
    "        \"    num_sentences = len(sentences)\\n\",\n",
    "        \"# total words in the file\\n\",\n",
    "        \"    words = [word  for word in text.split() if word.lower() not in stopwords ]\\n\",\n",
    "        \"    num_words = len(words)\\n\",\n",
    "        \" \\n\",\n",
    "        \"# complex words having syllable count is greater than 2\\n\",\n",
    "        \"# Complex words are words in the text that contain more than two syllables.\\n\",\n",
    "        \"    complex_words = []\\n\",\n",
    "        \"    for word in words:\\n\",\n",
    "        \"      vowels = 'aeiou'\\n\",\n",
    "        \"      syllable_count_word = sum( 1 for letter in word if letter.lower() in vowels)\\n\",\n",
    "        \"      if syllable_count_word > 2:\\n\",\n",
    "        \"        complex_words.append(word)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Syllable Count Per Word\\n\",\n",
    "        \"# We count the number of Syllables in each word of the text by counting the vowels present in each word.\\n\",\n",
    "        \"#  We also handle some exceptions like words ending with \\\"es\\\",\\\"ed\\\" by not counting them as a syllable.\\n\",\n",
    "        \"    syllable_count = 0\\n\",\n",
    "        \"    syllable_words =[]\\n\",\n",
    "        \"    for word in words:\\n\",\n",
    "        \"      if word.endswith('es'):\\n\",\n",
    "        \"        word = word[:-2]\\n\",\n",
    "        \"      elif word.endswith('ed'):\\n\",\n",
    "        \"        word = word[:-2]\\n\",\n",
    "        \"      vowels = 'aeiou'\\n\",\n",
    "        \"      syllable_count_word = sum( 1 for letter in word if letter.lower() in vowels)\\n\",\n",
    "        \"      if syllable_count_word >= 1:\\n\",\n",
    "        \"        syllable_words.append(word)\\n\",\n",
    "        \"        syllable_count += syllable_count_word\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"    avg_sentence_len = num_words / num_sentences\\n\",\n",
    "        \"    avg_syllable_word_count = syllable_count / len(syllable_words)\\n\",\n",
    "        \"    Percent_Complex_words  =  len(complex_words) / num_words\\n\",\n",
    "        \"    Fog_Index = 0.4 * (avg_sentence_len + Percent_Complex_words)\\n\",\n",
    "        \"\\n\",\n",
    "        \"    return avg_sentence_len, Percent_Complex_words, Fog_Index, len(complex_words),avg_syllable_word_count\\n\",\n",
    "        \"\\n\",\n",
    "        \"# iterate through each file or doc\\n\",\n",
    "        \"for file in os.listdir(text_dir):\\n\",\n",
    "        \"  x,y,z,a,b = measure(file)\\n\",\n",
    "        \"  avg_sentence_length.append(x)\\n\",\n",
    "        \"  Percentage_of_Complex_words.append(y)\\n\",\n",
    "        \"  Fog_Index.append(z)\\n\",\n",
    "        \"  complex_word_count.append(a)\\n\",\n",
    "        \"  avg_syllable_word_count.append(b)\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"F8RaMuD_EnQQ\"\n",
    "      },\n",
    "      \"execution_count\": 27,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"# Word Count and Average Word Length Sum of the total number of characters in each word/Total number of words\\n\",\n",
    "        \"# We count the total cleaned words present in the text by \\n\",\n",
    "        \"# removing the stop words (using stopwords class of nltk package).\\n\",\n",
    "        \"# removing any punctuations like ? ! , . from the word before counting.\\n\",\n",
    "        \"\\n\",\n",
    "        \"def cleaned_words(file):\\n\",\n",
    "        \"  with open(os.path.join(text_dir,file), 'r') as f:\\n\",\n",
    "        \"    text = f.read()\\n\",\n",
    "        \"    text = re.sub(r'[^\\\\w\\\\s]', '' , text)\\n\",\n",
    "        \"    words = [word  for word in text.split() if word.lower() not in stopwords]\\n\",\n",
    "        \"    length = sum(len(word) for word in words)\\n\",\n",
    "        \"    average_word_length = length / len(words)\\n\",\n",
    "        \"  return len(words),average_word_length\\n\",\n",
    "        \"\\n\",\n",
    "        \"word_count = []\\n\",\n",
    "        \"average_word_length = []\\n\",\n",
    "        \"for file in os.listdir(text_dir):\\n\",\n",
    "        \"  x, y = cleaned_words(file)\\n\",\n",
    "        \"  word_count.append(x)\\n\",\n",
    "        \"  average_word_length.append(y)\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"# To calculate Personal Pronouns mentioned in the text, we use regex to find \\n\",\n",
    "        \"# the counts of the words - “I,” “we,” “my,” “ours,” and “us”. Special care is taken\\n\",\n",
    "        \"#  so that the country name US is not included in the list.\\n\",\n",
    "        \"def count_personal_pronouns(file):\\n\",\n",
    "        \"  with open(os.path.join(text_dir,file), 'r') as f:\\n\",\n",
    "        \"    text = f.read()\\n\",\n",
    "        \"    personal_pronouns = [\\\"I\\\", \\\"we\\\", \\\"my\\\", \\\"ours\\\", \\\"us\\\"]\\n\",\n",
    "        \"    count = 0\\n\",\n",
    "        \"    for pronoun in personal_pronouns:\\n\",\n",
    "        \"      count += len(re.findall(r\\\"\\\\b\\\" + pronoun + r\\\"\\\\b\\\", text)) # \\\\b is used to match word boundaries\\n\",\n",
    "        \"  return count\\n\",\n",
    "        \"\\n\",\n",
    "        \"pp_count = []\\n\",\n",
    "        \"for file in os.listdir(text_dir):\\n\",\n",
    "        \"  x = count_personal_pronouns(file)\\n\",\n",
    "        \"  pp_count.append(x)\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"4NElx7d94ICm\"\n",
    "      },\n",
    "      \"execution_count\": 28,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"output_df = pd.read_excel('Output Data Structure.xlsx')\\n\",\n",
    "        \"\\n\",\n",
    "        \"# URL_ID 44 ,57, 144 does not exists i,e. page does not exist, throughs 404 error\\n\",\n",
    "        \"# so we are going to drop these rows from the table\\n\",\n",
    "        \"output_df.drop([44-37,57-37,144-37], axis = 0, inplace=True)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# These are the required parameters \\n\",\n",
    "        \"variables = [positive_score,\\n\",\n",
    "        \"            negative_score,\\n\",\n",
    "        \"            polarity_score,\\n\",\n",
    "        \"            subjectivity_score,\\n\",\n",
    "        \"            avg_sentence_length,\\n\",\n",
    "        \"            Percentage_of_Complex_words,\\n\",\n",
    "        \"            Fog_Index,\\n\",\n",
    "        \"            avg_sentence_length,\\n\",\n",
    "        \"            complex_word_count,\\n\",\n",
    "        \"            word_count,\\n\",\n",
    "        \"            avg_syllable_word_count,\\n\",\n",
    "        \"            pp_count,\\n\",\n",
    "        \"            average_word_length]\\n\",\n",
    "        \"\\n\",\n",
    "        \"# write the values to the dataframe\\n\",\n",
    "        \"for i, var in enumerate(variables):\\n\",\n",
    "        \"  output_df.iloc[:,i+2] = var\\n\",\n",
    "        \"\\n\",\n",
    "        \"#now save the dataframe to the disk\\n\",\n",
    "        \"output_df.to_csv('Output_Data.csv')\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"mXsnVluZ9TG3\"\n",
    "      },\n",
    "      \"execution_count\": 29,\n",
    "      \"outputs\": []\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
